{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1P2RtOu6GqsoX4GNhVZNSBRLnjnrlyo-Q","timestamp":1761214386703}],"authorship_tag":"ABX9TyOGagck+8L1TAuTgLCGwVlX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# --- Step 1: Install and Import Libraries ---\n","print(\"Installing and importing libraries...\")\n","!pip install pandas nltk matplotlib seaborn\n","import pandas as pd\n","import nltk\n","import string\n","import re\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from collections import Counter\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.util import bigrams\n","import os\n","\n","# Download necessary NLTK resources\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","print(\"--- Libraries ready! ---\")\n","\n","\n","# --- Step 2: Load and Prepare Data ---\n","print(\"\\n--- Step 2: Loading Data ---\")\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# !! IMPORTANT: Update the path to your file !!\n","file_path = '/content/drive/MyDrive/all_musk_posts.csv'\n","\n","try:\n","    df = pd.read_csv(file_path, low_memory=False)\n","    df.columns = df.columns.str.strip() # Clean column names\n","    # Ensure the text column has no missing values (NaN)\n","    df['fullText'] = df['fullText'].fillna('')\n","    print(f\"Loaded {len(df)} tweets.\")\n","except Exception as e:\n","    print(f\"ERROR: Could not load file. Check your path. Error: {e}\")\n","\n","# --- Step 3: Define Our Targets (from the \"Rosetta Stone\") ---\n","# We define what we are looking for to support our README analysis\n","TARGET_AFFIRMATIONS = ['yeah', 'true', 'yup']\n","TARGET_THEMES = ['tesla', 'media']\n","TARGET_BIGRAMS = [('coming', 'soon'), ('legacy', 'media')]\n","\n","\n","# --- Step 4: Analysis 1 - Affirmations (\"Yeah\", \"True\", \"Yup\") ---\n","# For this, we need very light cleaning. We DO NOT remove stopwords.\n","print(\"\\n--- Step 4: Analyzing Affirmations (Yeah, True, Yup) ---\")\n","\n","def tokenize_raw(text):\n","    text = str(text).lower() # Only lowercase\n","    text = text.translate(str.maketrans('', '', string.punctuation)) # Remove punctuation\n","    tokens = word_tokenize(text)\n","    return tokens\n","\n","# Count all words from lightly cleaned text\n","raw_word_counts = Counter()\n","df['fullText'].apply(lambda x: raw_word_counts.update(tokenize_raw(x)))\n","\n","# Pull the results for our targets\n","affirmation_results = {}\n","for word in TARGET_AFFIRMATIONS:\n","    count = raw_word_counts[word]\n","    affirmation_results[word] = count\n","    print(f\"Found '{word}': {count} times.\")\n","\n","\n","# --- Step 5: Analysis 2 - Thematic Words (\"Tesla\", \"media\") ---\n","# Here, we need full cleaning to remove \"noise\" (stopwords, links, etc.)\n","print(\"\\n--- Step 5: Analyzing Thematic Words (Tesla, media) ---\")\n","\n","stop_words = set(stopwords.words('english'))\n","custom_stopwords = {'rt', 'via', 'amp', 'elon', 'musk'}\n","stop_words.update(custom_stopwords)\n","\n","def clean_text(text):\n","    text = str(text).lower()\n","    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n","    text = re.sub(r'\\@\\w+', '', text)\n","    text = re.sub(r'\\#\\w+', '', text)\n","    text = text.translate(str.maketrans('', '', string.punctuation))\n","    text = re.sub(r'\\d+', '', text)\n","    tokens = word_tokenize(text)\n","    cleaned_tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n","    return cleaned_tokens\n","\n","# Count all words from fully cleaned text\n","themed_word_counts = Counter()\n","df['fullText'].apply(lambda x: themed_word_counts.update(clean_text(x)))\n","\n","# Pull the results for our targets\n","theme_results = {}\n","for word in TARGET_THEMES:\n","    count = themed_word_counts[word]\n","    theme_results[word] = count\n","    print(f\"Found '{word}': {count} times.\")\n","\n","\n","# --- Step 6: Analysis 3 - Bigrams (\"coming soon\", \"legacy media\") ---\n","# We look for two-word phrases in the cleaned text\n","print(\"\\n--- Step 6: Analyzing Bigrams (coming soon, legacy media) ---\")\n","\n","all_cleaned_tokens = []\n","df['fullText'].apply(lambda x: all_cleaned_tokens.extend(clean_text(x)))\n","\n","bigram_counts = Counter(bigrams(all_cleaned_tokens))\n","\n","# Pull the results for our targets\n","bigram_results = {}\n","for bigram in TARGET_BIGRAMS:\n","    count = bigram_counts[bigram]\n","    bigram_results[bigram] = count\n","    print(f\"Found '{' '.join(bigram)}': {count} times.\")\n","\n","# --- Step 7: Final Visualization (Data for the \"Rosetta Stone\") ---\n","# We create one chart that summarizes ALL our key findings\n","print(\"\\n--- Step 7: Creating final summary chart ---\")\n","\n","# Prepare data for the chart\n","report_data = {\n","    'yeah': affirmation_results['yeah'],\n","    'true': affirmation_results['true'],\n","    'yup': affirmation_results['yup'],\n","    'tesla': theme_results['tesla'],\n","    'media': theme_results['media'],\n","    'coming soon': bigram_results[('coming', 'soon')],\n","    'legacy media': bigram_results[('legacy', 'media')]\n","}\n","\n","df_report = pd.DataFrame(list(report_data.items()), columns=['Keyword', 'Frequency'])\n","df_report = df_report.sort_values(by='Frequency', ascending=False)\n","\n","# Create 'images' folder if it doesn't exist\n","os.makedirs('images', exist_ok=True)\n","\n","# Create the plot\n","plt.figure(figsize=(12, 8))\n","sns.barplot(x='Frequency', y='Keyword', data=df_report, palette='mako')\n","plt.title('Frequency of Key \"Rosetta Stone\" Terms in Musk\\'s Tweets', fontsize=16)\n","plt.xlabel('Total Frequency Count', fontsize=12)\n","plt.ylabel('Keyword / Phrase', fontsize=12)\n","\n","# Save the plot\n","plt.savefig('images/rosetta_stone_keywords.png')\n","print(\"Saved final chart to 'images/rosetta_stone_keywords.png'\")\n","plt.show()\n","\n","# Ta linia zosta≈Ça naprawiona:\n","print('\\n--- \"Rosetta Stone\" Analysis Complete! ---')\n","\n","\n"],"metadata":{"id":"JLbMzd1VjdJx"},"execution_count":null,"outputs":[]}]}